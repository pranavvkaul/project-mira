{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../core/resource.mjs\";\nimport { CursorPage } from \"../../core/pagination.mjs\";\nimport { buildHeaders } from \"../../internal/headers.mjs\";\nimport { sleep } from \"../../internal/utils/sleep.mjs\";\nimport { allSettledWithThrow } from \"../../lib/Util.mjs\";\nimport { path } from \"../../internal/utils/path.mjs\";\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(vectorStoreID, body, options) {\n    return this._client.post(path`/vector_stores/${vectorStoreID}/file_batches`, {\n      body,\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.get(path`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.post(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(batchID, params, options) {\n    const {\n      vector_store_id,\n      ...query\n    } = params;\n    return this._client.getAPIList(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`, CursorPage, {\n      query,\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(vectorStoreID, batchID, options) {\n    const headers = buildHeaders([options?.headers, {\n      'X-Stainless-Poll-Helper': 'true',\n      'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined\n    }]);\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(batchID, {\n        vector_store_id: vectorStoreID\n      }, {\n        ...options,\n        headers\n      }).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(vectorStoreId, {\n    files,\n    fileIds = []\n  }, options) {\n    if (files == null || files.length == 0) {\n      throw new Error(`No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`);\n    }\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}","map":{"version":3,"names":["APIResource","CursorPage","buildHeaders","sleep","allSettledWithThrow","path","FileBatches","create","vectorStoreID","body","options","_client","post","headers","retrieve","batchID","params","vector_store_id","get","cancel","createAndPoll","vectorStoreId","batch","poll","id","listFiles","query","getAPIList","pollIntervalMs","toString","undefined","data","response","withResponse","status","sleepInterval","headerInterval","headerIntervalMs","parseInt","isNaN","uploadAndPoll","files","fileIds","length","Error","configuredConcurrency","maxConcurrency","concurrencyLimit","Math","min","client","fileIterator","values","allFileIds","processFiles","iterator","item","fileObj","file","purpose","push","workers","Array","fill","map","file_ids"],"sources":["C:\\Users\\Prana\\Desktop\\mira\\frontend\\node_modules\\openai\\src\\resources\\vector-stores\\file-batches.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../core/resource';\nimport * as FilesAPI from './files';\nimport { VectorStoreFilesPage } from './files';\nimport * as VectorStoresAPI from './vector-stores';\nimport { APIPromise } from '../../core/api-promise';\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\nimport { buildHeaders } from '../../internal/headers';\nimport { RequestOptions } from '../../internal/request-options';\nimport { sleep } from '../../internal/utils/sleep';\nimport { type Uploadable } from '../../uploads';\nimport { allSettledWithThrow } from '../../lib/Util';\nimport { path } from '../../internal/utils/path';\n\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(\n    vectorStoreID: string,\n    body: FileBatchCreateParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    return this._client.post(path`/vector_stores/${vectorStoreID}/file_batches`, {\n      body,\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(\n    batchID: string,\n    params: FileBatchRetrieveParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    const { vector_store_id } = params;\n    return this._client.get(path`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(\n    batchID: string,\n    params: FileBatchCancelParams,\n    options?: RequestOptions,\n  ): APIPromise<VectorStoreFileBatch> {\n    const { vector_store_id } = params;\n    return this._client.post(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\n    });\n  }\n\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(\n    batchID: string,\n    params: FileBatchListFilesParams,\n    options?: RequestOptions,\n  ): PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\n    const { vector_store_id, ...query } = params;\n    return this._client.getAPIList(\n      path`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`,\n      CursorPage<FilesAPI.VectorStoreFile>,\n      { query, ...options, headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]) },\n    );\n  }\n\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(\n    vectorStoreID: string,\n    batchID: string,\n    options?: RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const headers = buildHeaders([\n      options?.headers,\n      {\n        'X-Stainless-Poll-Helper': 'true',\n        'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined,\n      },\n    ]);\n\n    while (true) {\n      const { data: batch, response } = await this.retrieve(\n        batchID,\n        { vector_store_id: vectorStoreID },\n        {\n          ...options,\n          headers,\n        },\n      ).withResponse();\n\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(\n    vectorStoreId: string,\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\n    options?: RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\n  ): Promise<VectorStoreFileBatch> {\n    if (files == null || files.length == 0) {\n      throw new Error(\n        `No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`,\n      );\n    }\n\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds: string[] = [...fileIds];\n\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds,\n    });\n  }\n}\n\n/**\n * A batch of files attached to a vector store.\n */\nexport interface VectorStoreFileBatch {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store files batch was\n   * created.\n   */\n  created_at: number;\n\n  file_counts: VectorStoreFileBatch.FileCounts;\n\n  /**\n   * The object type, which is always `vector_store.file_batch`.\n   */\n  object: 'vector_store.files_batch';\n\n  /**\n   * The status of the vector store files batch, which can be either `in_progress`,\n   * `completed`, `cancelled` or `failed`.\n   */\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\n\n  /**\n   * The ID of the\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  vector_store_id: string;\n}\n\nexport namespace VectorStoreFileBatch {\n  export interface FileCounts {\n    /**\n     * The number of files that where cancelled.\n     */\n    cancelled: number;\n\n    /**\n     * The number of files that have been processed.\n     */\n    completed: number;\n\n    /**\n     * The number of files that have failed to process.\n     */\n    failed: number;\n\n    /**\n     * The number of files that are currently being processed.\n     */\n    in_progress: number;\n\n    /**\n     * The total number of files.\n     */\n    total: number;\n  }\n}\n\nexport interface FileBatchCreateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format, and\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\n   * length of 64 characters. Values are strings with a maximum length of 512\n   * characters, booleans, or numbers.\n   */\n  attributes?: { [key: string]: string | number | boolean } | null;\n\n  /**\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n   * strategy. Only applicable if `file_ids` is non-empty.\n   */\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\n\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the vector store should use. Useful for tools like `file_search` that can access\n   * files. If `attributes` or `chunking_strategy` are provided, they will be applied\n   * to all files in the batch. Mutually exclusive with `files`.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * A list of objects that each include a `file_id` plus optional `attributes` or\n   * `chunking_strategy`. Use this when you need to override metadata for specific\n   * files. The global `attributes` or `chunking_strategy` will be ignored and must\n   * be specified for each file. Mutually exclusive with `file_ids`.\n   */\n  files?: Array<FileBatchCreateParams.File>;\n}\n\nexport namespace FileBatchCreateParams {\n  export interface File {\n    /**\n     * A [File](https://platform.openai.com/docs/api-reference/files) ID that the\n     * vector store should use. Useful for tools like `file_search` that can access\n     * files.\n     */\n    file_id: string;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format, and\n     * querying for objects via API or the dashboard. Keys are strings with a maximum\n     * length of 64 characters. Values are strings with a maximum length of 512\n     * characters, booleans, or numbers.\n     */\n    attributes?: { [key: string]: string | number | boolean } | null;\n\n    /**\n     * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n     * strategy. Only applicable if `file_ids` is non-empty.\n     */\n    chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\n  }\n}\n\nexport interface FileBatchRetrieveParams {\n  /**\n   * The ID of the vector store that the file batch belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileBatchCancelParams {\n  /**\n   * The ID of the vector store that the file batch belongs to.\n   */\n  vector_store_id: string;\n}\n\nexport interface FileBatchListFilesParams extends CursorPageParams {\n  /**\n   * Path param: The ID of the vector store that the files belong to.\n   */\n  vector_store_id: string;\n\n  /**\n   * Query param: A cursor for use in pagination. `before` is an object ID that\n   * defines your place in the list. For instance, if you make a list request and\n   * receive 100 objects, starting with obj_foo, your subsequent call can include\n   * before=obj_foo in order to fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Query param: Filter by file status. One of `in_progress`, `completed`, `failed`,\n   * `cancelled`.\n   */\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\n\n  /**\n   * Query param: Sort order by the `created_at` timestamp of the objects. `asc` for\n   * ascending order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport declare namespace FileBatches {\n  export {\n    type VectorStoreFileBatch as VectorStoreFileBatch,\n    type FileBatchCreateParams as FileBatchCreateParams,\n    type FileBatchRetrieveParams as FileBatchRetrieveParams,\n    type FileBatchCancelParams as FileBatchCancelParams,\n    type FileBatchListFilesParams as FileBatchListFilesParams,\n  };\n}\n\nexport { type VectorStoreFilesPage };\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;SAKbC,UAAU,QAAsC;SAChDC,YAAY,QAAE;SAEdC,KAAK,QAAE;SAEPC,mBAAmB,QAAE;SACrBC,IAAI,QAAE;AAEf,OAAM,MAAOC,WAAY,SAAQN,WAAW;EAC1C;;;EAGAO,MAAMA,CACJC,aAAqB,EACrBC,IAA2B,EAC3BC,OAAwB;IAExB,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAACP,IAAI,kBAAkBG,aAAa,eAAe,EAAE;MAC3EC,IAAI;MACJ,GAAGC,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;EAGAC,QAAQA,CACNC,OAAe,EACfC,MAA+B,EAC/BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACO,GAAG,CAACb,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,EAAE,EAAE;MACvF,GAAGL,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;;EAIAM,MAAMA,CACJJ,OAAe,EACfC,MAA6B,EAC7BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACC,IAAI,CAACP,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,SAAS,EAAE;MAC/F,GAAGL,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;EAGA,MAAMO,aAAaA,CACjBC,aAAqB,EACrBZ,IAA2B,EAC3BC,OAAsD;IAEtD,MAAMY,KAAK,GAAG,MAAM,IAAI,CAACf,MAAM,CAACc,aAAa,EAAEZ,IAAI,CAAC;IACpD,OAAO,MAAM,IAAI,CAACc,IAAI,CAACF,aAAa,EAAEC,KAAK,CAACE,EAAE,EAAEd,OAAO,CAAC;EAC1D;EAEA;;;EAGAe,SAASA,CACPV,OAAe,EACfC,MAAgC,EAChCN,OAAwB;IAExB,MAAM;MAAEO,eAAe;MAAE,GAAGS;IAAK,CAAE,GAAGV,MAAM;IAC5C,OAAO,IAAI,CAACL,OAAO,CAACgB,UAAU,CAC5BtB,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,QAAQ,EACrEd,UAAoC,EACpC;MAAEyB,KAAK;MAAE,GAAGhB,OAAO;MAAEG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;IAAC,CAAE,CACrG;EACH;EAEA;;;;;;EAMA,MAAMU,IAAIA,CACRf,aAAqB,EACrBO,OAAe,EACfL,OAAsD;IAEtD,MAAMG,OAAO,GAAGX,YAAY,CAAC,CAC3BQ,OAAO,EAAEG,OAAO,EAChB;MACE,yBAAyB,EAAE,MAAM;MACjC,kCAAkC,EAAEH,OAAO,EAAEkB,cAAc,EAAEC,QAAQ,EAAE,IAAIC;KAC5E,CACF,CAAC;IAEF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI,EAAET,KAAK;QAAEU;MAAQ,CAAE,GAAG,MAAM,IAAI,CAAClB,QAAQ,CACnDC,OAAO,EACP;QAAEE,eAAe,EAAET;MAAa,CAAE,EAClC;QACE,GAAGE,OAAO;QACVG;OACD,CACF,CAACoB,YAAY,EAAE;MAEhB,QAAQX,KAAK,CAACY,MAAM;QAClB,KAAK,aAAa;UAChB,IAAIC,aAAa,GAAG,IAAI;UAExB,IAAIzB,OAAO,EAAEkB,cAAc,EAAE;YAC3BO,aAAa,GAAGzB,OAAO,CAACkB,cAAc;UACxC,CAAC,MAAM;YACL,MAAMQ,cAAc,GAAGJ,QAAQ,CAACnB,OAAO,CAACK,GAAG,CAAC,sBAAsB,CAAC;YACnE,IAAIkB,cAAc,EAAE;cAClB,MAAMC,gBAAgB,GAAGC,QAAQ,CAACF,cAAc,CAAC;cACjD,IAAI,CAACG,KAAK,CAACF,gBAAgB,CAAC,EAAE;gBAC5BF,aAAa,GAAGE,gBAAgB;cAClC;YACF;UACF;UACA,MAAMlC,KAAK,CAACgC,aAAa,CAAC;UAC1B;QACF,KAAK,QAAQ;QACb,KAAK,WAAW;QAChB,KAAK,WAAW;UACd,OAAOb,KAAK;MAChB;IACF;EACF;EAEA;;;;;EAKA,MAAMkB,aAAaA,CACjBnB,aAAqB,EACrB;IAAEoB,KAAK;IAAEC,OAAO,GAAG;EAAE,CAA+C,EACpEhC,OAA+E;IAE/E,IAAI+B,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACE,MAAM,IAAI,CAAC,EAAE;MACtC,MAAM,IAAIC,KAAK,CACb,gHAAgH,CACjH;IACH;IAEA,MAAMC,qBAAqB,GAAGnC,OAAO,EAAEoC,cAAc,IAAI,CAAC;IAE1D;IACA,MAAMC,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAACJ,qBAAqB,EAAEJ,KAAK,CAACE,MAAM,CAAC;IAEtE,MAAMO,MAAM,GAAG,IAAI,CAACvC,OAAO;IAC3B,MAAMwC,YAAY,GAAGV,KAAK,CAACW,MAAM,EAAE;IACnC,MAAMC,UAAU,GAAa,CAAC,GAAGX,OAAO,CAAC;IAEzC;IACA;IACA,eAAeY,YAAYA,CAACC,QAAsC;MAChE,KAAK,IAAIC,IAAI,IAAID,QAAQ,EAAE;QACzB,MAAME,OAAO,GAAG,MAAMP,MAAM,CAACT,KAAK,CAAClC,MAAM,CAAC;UAAEmD,IAAI,EAAEF,IAAI;UAAEG,OAAO,EAAE;QAAY,CAAE,EAAEjD,OAAO,CAAC;QACzF2C,UAAU,CAACO,IAAI,CAACH,OAAO,CAACjC,EAAE,CAAC;MAC7B;IACF;IAEA;IACA,MAAMqC,OAAO,GAAGC,KAAK,CAACf,gBAAgB,CAAC,CAACgB,IAAI,CAACZ,YAAY,CAAC,CAACa,GAAG,CAACV,YAAY,CAAC;IAE5E;IACA,MAAMlD,mBAAmB,CAACyD,OAAO,CAAC;IAElC,OAAO,MAAM,IAAI,CAACzC,aAAa,CAACC,aAAa,EAAE;MAC7C4C,QAAQ,EAAEZ;KACX,CAAC;EACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}